"""
æµ‹è¯•å‘é‡åŒ–é›†æˆï¼ˆå®Œæ•´æµç¨‹ï¼‰

æ­¥éª¤ï¼š
1. åˆå§‹åŒ– bge-m3 æ¨¡å‹
2. åˆå§‹åŒ– Qdrant é›†åˆï¼ˆæ··åˆå‘é‡ï¼‰
3. æå– PDF å†…å®¹
4. å‘é‡åŒ–å¹¶å¯¼å…¥åˆ° Qdrant
5. æµ‹è¯•æœç´¢
"""
from pathlib import Path
from app.utils.unTaggedPDF.pdf_content_extractor import PDFContentExtractor
from app.utils.db.qdrant import QdrantUtil
from app.utils.db.qdrant.EmbeddingUtil import get_embedding_util
from insert_tender_chunks_with_embedding import insert_tender_chunks_from_memory_v2


def main():
    # é…ç½®
    task_id = "é„‚å°”å¤šæ–¯å¸‚æ”¿åºœç½‘ç«™ç¾¤é›†çº¦åŒ–å¹³å°å‡çº§æ”¹é€ é¡¹ç›®"
    base_dir = Path(r"E:\programFile\AIProgram\docxServer\pdf\task\é„‚å°”å¤šæ–¯å¸‚æ”¿åºœç½‘ç«™ç¾¤é›†çº¦åŒ–å¹³å°å‡çº§æ”¹é€ é¡¹ç›®")
    pdf_path = base_dir / f"{task_id}.pdf"

    if not pdf_path.exists():
        print(f"é”™è¯¯: PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    print("=" * 80)
    print("æ­¥éª¤ 1/5: åˆå§‹åŒ– bge-m3 æ¨¡å‹")
    print("=" * 80)

    # è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"  æ£€æµ‹åˆ°è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"  GPU: {torch.cuda.get_device_name(0)}")

    embedding_util = get_embedding_util(
        model_name='BAAI/bge-m3',
        use_fp16=True,
        device=device
    )

    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2/5: åˆå§‹åŒ– Qdrant é›†åˆ")
    print("=" * 80)

    qdrant_util = QdrantUtil(url="http://localhost:6333")

    # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
    existing_collections = qdrant_util.list_collections()
    if "tender_chunks" in existing_collections:
        print("  é›†åˆå·²å­˜åœ¨")
        response = input("  æ˜¯å¦åˆ é™¤å¹¶é‡æ–°åˆ›å»º? (y/n): ")
        if response.lower() == 'y':
            print("  æ­£åœ¨åˆ é™¤...")
            qdrant_util.client.delete_collection("tender_chunks")
            print("  æ­£åœ¨é‡æ–°åˆ›å»º...")
            qdrant_util.init_tender_collection(use_hybrid=True)
    else:
        print("  æ­£åœ¨åˆ›å»ºé›†åˆ...")
        qdrant_util.init_tender_collection(use_hybrid=True)

    print("\n" + "=" * 80)
    print("æ­¥éª¤ 3/5: æå– PDF å†…å®¹")
    print("=" * 80)

    extractor = PDFContentExtractor(
        str(pdf_path),
        enable_cross_page_merge=True,
        verbose=False
    )

    # æå–è¡¨æ ¼å’Œæ®µè½ï¼ˆå†…å­˜ï¼‰
    print("  æå–è¡¨æ ¼...")
    tables_result = extractor.extract_all_tables()
    tables_full = tables_result.get("tables", [])
    print(f"    âœ“ æå–äº† {len(tables_full)} ä¸ªè¡¨æ ¼")

    print("  æå–æ®µè½...")
    paragraphs_result = extractor.extract_all_paragraphs()
    paragraphs_full = paragraphs_result.get("paragraphs", [])
    print(f"    âœ“ æå–äº† {len(paragraphs_full)} ä¸ªæ®µè½")

    # ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå–å‰å‡ ä¸ªæ®µè½å’Œè¡¨æ ¼
    MAX_PARAGRAPHS = 10  # åªå–å‰10ä¸ªæ®µè½
    MAX_TABLE_ROWS = 5   # æ¯ä¸ªè¡¨æ ¼åªå–å‰5è¡Œ

    print(f"\n  [æµ‹è¯•æ¨¡å¼] åªå‘é‡åŒ–å‰ {MAX_PARAGRAPHS} ä¸ªæ®µè½å’Œæ¯ä¸ªè¡¨æ ¼å‰ {MAX_TABLE_ROWS} è¡Œ")
    paragraphs = paragraphs_full[:MAX_PARAGRAPHS]

    # é™åˆ¶è¡¨æ ¼è¡Œæ•°
    tables = []
    for table in tables_full:
        limited_table = table.copy()
        rows = table.get("rows", [])
        limited_table["rows"] = rows[:MAX_TABLE_ROWS]
        tables.append(limited_table)

    print(f"    â†’ å°†å‘é‡åŒ–: {len(paragraphs)} ä¸ªæ®µè½")
    total_rows = sum(len(t.get("rows", [])) for t in tables)
    print(f"    â†’ å°†å‘é‡åŒ–: {total_rows} ä¸ªè¡¨æ ¼è¡Œï¼ˆæ¥è‡ª {len(tables)} ä¸ªè¡¨æ ¼ï¼‰")

    print("\n" + "=" * 80)
    print("æ­¥éª¤ 4/5: å‘é‡åŒ–å¹¶å¯¼å…¥åˆ° Qdrant")
    print("=" * 80)

    doc_id = f"ORDOS-{task_id[:10]}"
    metadata = {
        "region": "å†…è’™å¤-é„‚å°”å¤šæ–¯",
        "agency": "é„‚å°”å¤šæ–¯å¸‚æ”¿åºœ",
        "published_at_ts": 1706601600
    }

    chunk_count = insert_tender_chunks_from_memory_v2(
        qdrant_util=qdrant_util,
        doc_id=doc_id,
        tables=tables,
        paragraphs=paragraphs,
        embedding_util=embedding_util,
        metadata=metadata
    )

    print(f"\n  âœ“ æˆåŠŸå¯¼å…¥ {chunk_count} ä¸ª chunks")

    # æ˜¾ç¤ºæœ€ç»ˆç¼“å­˜ç»Ÿè®¡
    stats = embedding_util.get_cache_stats()
    print(f"\n  [ç¼“å­˜ç»Ÿè®¡]")
    print(f"    ç¼“å­˜å¤§å°: {stats['cache_size']}")
    print(f"    ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
    print(f"    ç¼“å­˜æœªå‘½ä¸­: {stats['cache_misses']}")
    print(f"    å‘½ä¸­ç‡: {stats['hit_rate']}")

    print("\n" + "=" * 80)
    print("æ­¥éª¤ 5/5: æµ‹è¯•æœç´¢")
    print("=" * 80)

    # æµ‹è¯•æŸ¥è¯¢
    query = "ç½‘ç«™é›†çº¦åŒ–å¹³å°"
    print(f"\n  æŸ¥è¯¢: '{query}'")

    dense_vec, sparse_vec = embedding_util.encode_query(query)

    # Qdrant 1.9.0: ä»…ä½¿ç”¨ç¨ å¯†å‘é‡æœç´¢ (1.9.0 ä¸æ”¯æŒå®¢æˆ·ç«¯ sparse æœç´¢)
    print(f"  [æœç´¢] ä½¿ç”¨ç¨ å¯†å‘é‡æœç´¢...")
    results = qdrant_util.client.search(
        collection_name="tender_chunks",
        query_vector=("dense", dense_vec),
        limit=5,
        with_payload=True
    )

    print(f"\n  æœç´¢ç»“æœï¼ˆTop 5ï¼‰:")
    for i, result in enumerate(results, 1):
        payload = result.payload
        content = payload['content'][:80]
        print(f"\n    [{i}] ç›¸ä¼¼åº¦: {result.score:.4f}")
        print(f"        ç±»å‹: {payload['chunk_type']}, é¡µç : {payload['page']}")
        print(f"        å†…å®¹: {content}...")

    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 80)


if __name__ == '__main__':
    main()
